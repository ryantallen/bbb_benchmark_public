# BBB benchmark — Back Bay Battery AI Strategy Benchmarker

This repo automates end-to-end benchmarking of LLMs on Harvard Business School Publishing’s **Back Bay Battery** strategy simulation:

- Run many simulation plays automatically (Selenium + HBSP accounts)
- Save per-run results to JSON
- Convert JSON → a tidy CSV
- Generate the figures used for the accompanying paper/site
- **Read the paper**: [How Well Can AI do Strategy? Empirical Benchmarking Using Strategy Simulations](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5239555)
- **Latest benchmarks site**: `https://ryantallen.github.io/bbb_benchmark_public/`

## What’s included (and what’s not)

This repository is shared **without**:

- **`raw_results/`**: generated JSON outputs from runs (**not committed**; ignored by git)
- **`logs/`**: generated execution logs (**not committed**; ignored by git)
- **Human/MBA benchmark scores**: the optional `past_mba_performance.csv` is **not included** in the public repo because we did not have authorization to share these scores.

If you generate results locally, you’ll see `raw_results/` and `logs/` appear at the repo root. They’re intentionally ignored to avoid committing large/private artifacts.

## Requirements

- Python **3.10+**
- Google Chrome installed (Selenium drives Chrome)
- HBSP access to the Back Bay Battery simulation via a **coursepack**
- API keys for whichever LLM providers you run (OpenAI/Anthropic/Gemini/xAI/OpenRouter, etc.)

Install Python packages:

```bash
pip install -r requirements.txt
```

## HBSP setup (two accounts)

The automation expects **two HBSP accounts**:

- **STUDENT account** (plays the simulation during runs)
  - Env vars: `HBSP_USERNAME`, `HBSP_PASSWORD`
- **ADMIN/Teacher account** (resets/configures the simulation between runs and scrapes final results)
  - Env vars: `ADMIN_USERNAME`, `ADMIN_PASSWORD`

You also need your coursepack URL:

- **`HBSP_COURSEPACK_URL`**: the full HBSP coursepack URL that contains Back Bay Battery (this is effectively your “course number/id” link on HBSP)
  - The **ADMIN** account should own/manage this coursepack
  - The **STUDENT** account must be enrolled and able to launch the simulation from that same coursepack

## Create your `.env`

Create a `.env` file in the repo root:

```env
# STUDENT account (used to play the simulation)
HBSP_USERNAME=your_student_username
HBSP_PASSWORD=your_student_password

# ADMIN/Teacher account (used to configure/reset between runs)
ADMIN_USERNAME=your_admin_username
ADMIN_PASSWORD=your_admin_password

# Back Bay Battery coursepack URL managed by the ADMIN account
HBSP_COURSEPACK_URL=https://hbsp.harvard.edu/coursepacks/XXXXXXXX

# API keys (set the ones you need)
OPENAI_API_KEY=...
ANTHROPIC_API_KEY=...
GEMINI_API_KEY=...
XAI_API_KEY=...
OPENROUTER_API_KEY=...
```

Notes:

- `simulation/hbsp_login.py` loads `.env` automatically via `python-dotenv`.
- `simulation/backbay_main.py` uses the STUDENT creds to play; `simulation/reset_backbay.py` uses the ADMIN creds to reset/collect final tables.

## Project structure

```
bbb_benchmark_public/
├── simulation/
│   ├── backbay_main.py           # Main experiment driver (edit configurations here)
│   ├── backbay_driver.py         # Selenium automation of yearly decisions
│   ├── hbsp_login.py             # HBSP login + launch flow
│   ├── reset_backbay.py          # Reset + scrape final "Best Scores" + "Simulation Summary"
│   ├── masking.py                # Prompt masking utilities
│   ├── background_masked.txt
│   └── format_instructions.txt
├── raw_results/                  # GENERATED (ignored): JSON outputs per run/provider
├── logs/                         # GENERATED (ignored): execution logs
├── analysis/
│   ├── json_parser.py            # Convert raw_results JSON → analysis/data/tidy_results.csv
│   ├── plots.py                  # Main plot suite (writes analysis/figures/*.png)
│   ├── plot_over_time.py         # Composite vs release-date plots (writes analysis/figures/*.png)
│   ├── data/
│   │   └── tidy_results.csv      # GENERATED by json_parser.py
│   └── figures/                  # GENERATED plots
└── requirements.txt
```

## Run the simulation

1) **Choose models / settings** by editing `simulation/backbay_main.py` and uncommenting/adding entries in `configurations`.

Each entry is:

`[model, num_runs, instruction_set, background_set, mask_mode, difficulty]`

Example:

```python
configurations = [
    ["openai/gpt-4o-2024-08-06", 5, "default_instructions", "masked_background", "masked", "Basic"],
    ["openai/gpt-4o-2024-08-06", 5, "default_instructions", "masked_background", "masked", "Advanced"],
]
```

2) **Run experiments** (headless optional):

```bash
python simulation/backbay_main.py --headless
```

Outputs (generated locally):

- `raw_results/` (JSON per model run)
- `logs/` (txt logs per run)

## Build the analysis dataset (JSON → CSV)

From the repo root:

```bash
python analysis/json_parser.py
```

This writes:

- `analysis/data/tidy_results.csv`

`analysis/json_parser.py` expects a directory structure like `raw_results/all_results_<provider>/**/*.json` (it also has special handling for `all_results_openrouter/`).

## Make plots

From the repo root:

```bash
python analysis/plots.py
python analysis/plot_over_time.py
```

Outputs:

- `analysis/figures/*.png`

### Optional: Human/MBA benchmarks

`analysis/plots.py` and `analysis/plot_over_time.py` will **automatically include** MBA reference lines if a CSV is present locally. The public repo does not ship this file.

If you have permission to use it, place it at one of:

- `analysis/data/past_mba_performance.csv`
- `analysis/inputs/past_mba_performance.csv`

## Troubleshooting

- **HBSP permissions**: both accounts must be able to access the same coursepack; the ADMIN must be able to “Manage Simulation” to reset runs.
- **Popups/overlays**: the login flow blocks/dismisses common Qualtrics overlays, but HBSP UI changes can break selectors.
- **First run is slow**: `webdriver-manager` may download a ChromeDriver build the first time.

## License

Apache license 2.0 — see `LICENSE`.
